# countURL
URL计数问题

## 问题：100G的url文件，使用1G内存得出频率最高的100个url和数量

## 解题思路：

### 一、分解问题

1、如何使用1G内存处理100G的文件

2、如何在内存中对url做处理

3、如何找出前100个并将对应的url和数量打印出来

### 二、解决方法
1、将100G大文件分解成多个小文件

2、既能存储查询串，又能存储查询串的出现次数，使用hashmap，map节点为URL和count，将每个小文件的URL都用该hashmap进行计数

3、用小堆获取到最大的100个值

### 三、具体做法
1、
（1）由于没有100G的url文件，电脑也没有这么大的硬盘，于是写了一个python脚本create.py来create url文件，url没有采取正常的url格式，而是使用 URL+随机数 的形式来代替，每两条url之间用空格隔开，脚本中生成了100000条url，文件大小约为1-2M，文件名为test1.txt。

（2）分割url文件，写了一个split.py来做分割。分割的做法是边读边写，从test1读一个字符，就在新文件写一个字符。设置了一个变量word来记录写入大小，写入10000字节后到下一条url前停止写入该文件，换新文件继续写，同时清空word，直到写完全部url。该分割过程使用的内存为word大小，最大约为10000字节。结果生成100+的txt文件
（这里是10000字节对应100000条url，若url为100g则同样放大该值）

2、
经过查阅资料，决定选择hashmap的方式对这些url做处理。因为看到了java里有直接的hashmap操作，所以决定使用java作为工作语言。一个一个地把分割后的txt读入内存，用hashmap的put方法将url和计数值count放入map中，一旦遇到相同的url，就将count+1。读完一个文件后关闭，读入下一个文件（这里有一个问题，hashmap的大小可能会超过1g，暂时没有想到解决方法），结束后就得到了全部的url和其对应的数量。
### ps:Main.java里面的第19行，n的值是小于等于最后一个文件的编号，需根据生成的文件数作修改

3、
得知了数据，找最大的100个，这里使用了小根堆。新建一个大小为100的小根堆，遍历map，每读入map中的一个count，就与堆顶的最小值作比较，比它大就将其替换，再找出最小值放置堆顶，最终找到100个，但是由于数组大小的限制，无法得出一些个数与第100个相同的url。同时，为了找到count对应的url，将堆的结构定为int和String，String绑定int一起操作，就能一起输出url和count。


## 运行环境
工作环境：Python 3.7.1

        javac 1.8.0_111

操作系统：macOS 10.14.5

### 运行

    $python3 create.py
    $python3 split.py
    $cd /src
    $javac Main.java
    $java Main

由于修改过ide上的路径，不排除其它环境中会报路径错误，可能需要手动修改